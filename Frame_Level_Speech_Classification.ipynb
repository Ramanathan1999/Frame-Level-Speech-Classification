{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ERgBpbcMmB"
      },
      "source": [
        "# HW1: Frame-Level Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLkH6GMGcWcE"
      },
      "source": [
        "In this homework, you will be working with MFCC data consisting of 15 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwYu9sSUnSho",
        "outputId": "830ab10d-9c2d-49f7-d2d1-907b18178895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 89.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 70.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 60.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 85.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 86.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 95.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 92.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 84.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 90.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 89.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummaryX wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI4qfx7tiBZt",
        "outputId": "c81de485-34e4-4c3f-c058-d294d6a83f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchsummaryX import summary\n",
        "import sklearn\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "import sklearn.metrics\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yBgXjKV1O0Z"
      },
      "outputs": [],
      "source": [
        "### If you are using colab, you can import google drive to save model checkpoints in a folder\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-9qE20hmCgQ"
      },
      "outputs": [],
      "source": [
        "### PHONEME LIST\n",
        "PHONEMES = [\n",
        "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIi0Big7vPa9"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBCbeRhixGM7"
      },
      "source": [
        "This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPBUd7Cnl-Rx",
        "outputId": "dc7b1dcd-c472-49b2-8289-5393eee434fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 1.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73275 sha256=a796cdbd0ec7b4ca70e6d5b4c65b3d5ee1303e6d681984598c62c86f8d9f8247\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/f7/d8/c3902cacb7e62cb611b1ad343d7cc07f42f7eb76ae3a52f3d1\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"ramanathan16\",\"key\":\"d4035f55a52974a7c4d6bae8054bc215\"}') \n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if2Somqfbje1",
        "outputId": "3598cc43-afc2-44fe-f70d-66fc5c00da58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-f22-hw1p2.zip to /content\n",
            "100% 2.13G/2.13G [00:37<00:00, 120MB/s]\n",
            "100% 2.13G/2.13G [00:37<00:00, 61.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# commands to download data from kaggle\n",
        "\n",
        "!kaggle competitions download -c 11-785-f22-hw1p2\n",
        "!mkdir '/content/data'\n",
        "\n",
        "!unzip -qo '11-785-f22-hw1p2.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuzce0_TdcaR"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_7QgMbBdgPp"
      },
      "source": [
        "This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n",
        "\n",
        "Before running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6jy83aDhmnT"
      },
      "outputs": [],
      "source": [
        "#from torch._C import int32\n",
        "# Dataset class to load train and validation data\n",
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, context, offset=0, partition= \"train\", limit=-1): # Feel free to add more arguments\n",
        "\n",
        "        self.context = context\n",
        "        self.offset = offset\n",
        "        self.data_path = data_path\n",
        "\n",
        "        if partition == \"train\":\n",
        "          self.mfcc_dir = self.data_path/ Path(partition+'-clean-100') /Path('mfcc') # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data\n",
        "          self.transcript_dir = self.data_path/ Path(partition+'-clean-100') /Path('transcript') # TODO: Transcripts directory - use partition to acces train/dev directories from kaggle data\n",
        "\n",
        "        elif partition == \"dev\":\n",
        "          self.mfcc_dir = self.data_path/ Path(partition +'-clean')/Path('mfcc')\n",
        "          self.transcript_dir = self.data_path/ Path(partition+'-clean')/Path('transcript')\n",
        "\n",
        "\n",
        "        mfcc_names = sorted(os.listdir(self.mfcc_dir)) # TODO: List files in sefl.mfcc_dir_dir using os.listdir in sorted order, optionally subset using limit to slice the number of files you load\n",
        "        transcript_names = sorted(os.listdir(self.transcript_dir)) # TODO: List files in self.transcript_dir using os.listdir in sorted order, optionally subset using limit to slice the number of files you load\n",
        "\n",
        "        assert len(mfcc_names) == len(transcript_names) # Making sure that we have the same no. of mfcc and transcripts\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "        b= ['<sos>','<eos>']\n",
        "\n",
        "        # TODO:\n",
        "        # Iterate through mfccs and transcripts\n",
        "        for i in range(0, len(mfcc_names)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc = np.load(self.mfcc_dir/mfcc_names[i])\n",
        "        #   Optionally do Cepstral Normalization of mfcc\n",
        "            \n",
        "            mean = np.mean(mfcc, axis=0)\n",
        "            var = np.var(mfcc,axis=0)\n",
        "            n_mfcc = (mfcc - mean)/var\n",
        "\n",
        "\n",
        "        #   Load the corresponding transcript\n",
        "            transcript = np.load(self.transcript_dir/transcript_names[i]) # Remove [SOS] and [EOS] from the transcript (Is there an efficient way to do this \n",
        "            # without traversing through the transcript?)\n",
        "            clean_transcript = np.delete(transcript, np.where(transcript == '<sos>'))\n",
        "            clean_transcript = np.delete(clean_transcript, np.where(clean_transcript == '<eos>'))\n",
        "\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(n_mfcc)\n",
        "            self.transcripts.append(clean_transcript)\n",
        "\n",
        "        \n",
        "\n",
        "        # NOTE:\n",
        "        # Each mfcc is of shape T1 x 15, T2 x 15, ...\n",
        "        # Each transcript is of shape (T1+2) x 15, (T2+2) x 15 before removing [SOS] and [EOS]\n",
        "\n",
        "        # TODO: Concatenate all mfccs in self.mfccs such that the final shape is T x 15 (Where T = T1 + T2 + ...) \n",
        "        self.mfccs = np.concatenate(self.mfccs,axis =0)\n",
        "\n",
        "        # TODO: Concatenate all transcripts in self.transcripts such that the final shape is (T,) meaning, each time step has one phoneme output\n",
        "        self.transcripts = np.concatenate(self.transcripts,axis =0)\n",
        "        # Hint: Use numpy to concatenate\n",
        "\n",
        "        # Take some time to think about what we have done. self.mfcc is an array of the format (Frames x Features). Our goal is to recognize phonemes of each frame\n",
        "        # From hw0, you will be knowing what context is. We can introduce context by padding zeros on top and bottom of self.mfcc\n",
        "\n",
        "        \n",
        "\n",
        "        # These are the available phonemes in the transcript\n",
        "        self.phonemes = [\n",
        "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n",
        "        # But the neural network cannot predict strings as such. Instead we map these phonemes to integers\n",
        "\n",
        "        map = {x: i for i,x in enumerate(self.phonemes)}\n",
        "\n",
        "        for i in range(len(self.transcripts)):\n",
        "            self.transcripts[i]= map[self.transcripts[i]]\n",
        "\n",
        "        self.transcripts = np.asarray(self.transcripts, dtype=int)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        # TODO: Map the phonemes to their corresponding list indexes in self.phonemes\n",
        "        # Now, if an element in self.transcript is 0, it means that it is 'SIL' (as per the above example)\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "        self.mfccs = np.pad(self.mfccs, pad_width=((self.context,self.context),(0,0)), mode=\"constant\", constant_values= 0)\n",
        "        self.transcripts= np.pad(self.transcripts, pad_width=((self.context, self.context)), mode=\"constant\", constant_values= 0)\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "      \n",
        "\n",
        "      start= ind\n",
        "      end = ind +(2*self.context+1)\n",
        "\n",
        "        \n",
        "      frames = self.mfccs[start:end] # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "      # After slicing, you get an array of shape 2*context+1 x 15. But our MLP needs 1d data and not 2d.\n",
        "      frames = frames.flatten() # TODO: Flatten to get 1d data\n",
        "\n",
        "      frames = torch.FloatTensor(frames) # Convert to tensors\n",
        "      phoneme = torch.tensor(self.transcripts[ind+self.context])       \n",
        "\n",
        "      return frames, phoneme\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "E0wO-YYOCQOf",
        "outputId": "92280c55-9591-4237-99b5-0238bc870341"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a21a249ee18d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-82326e7841ca>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, context, offset, partition, limit)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfcc_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#   Load a single mfcc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mmfcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfcc_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmfcc_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m#   Optionally do Cepstral Normalization of mfcc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 441\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0m_check_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m     \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfortran_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_array_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Cannot parse header: {!r}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36msafe_eval\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1002\u001b[0m     \u001b[0;31m# Local import to speed up numpy's import time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \"\"\"\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_or_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ast.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, filename, mode)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mEquivalent\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyCF_ONLY_AST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyCF_ONLY_AST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "test = AudioDataset(data_path='/content/data',context=1, offset=0, partition= \"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAO8FFtGLTSw",
        "outputId": "6a9f3ea3-89d6-4fbb-86f8-4206c56dfa79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 12.3430,\n",
            "        -1.1226, -0.1066,  0.0684, -0.2572, -0.1075,  0.2360, -0.2117, -0.1686,\n",
            "        -0.4367, -0.4411,  0.0822, -0.0688, -0.1083, -0.2555]), tensor(0))\n"
          ]
        }
      ],
      "source": [
        "print(test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8KfVP39S6o7"
      },
      "outputs": [],
      "source": [
        "class AudioTestDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data_path, context, offset=0, partition= \"test\", limit=-1):\n",
        "\n",
        "    self.context = context\n",
        "    self.offset = offset\n",
        "    self.data_path = data_path\n",
        "\n",
        "    self.mfcc_dir = self.data_path/ Path(partition+'-clean') /Path('mfcc') # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data\n",
        "    \n",
        "    mfcc_names = sorted(os.listdir(self.mfcc_dir)) \n",
        "   \n",
        "\n",
        "    self.mfccs = []\n",
        "\n",
        "    for i in range(0, len(mfcc_names)):\n",
        "        #   Load a single mfcc\n",
        "        mfcc = np.load(self.mfcc_dir/mfcc_names[i])\n",
        "        mean = np.mean(mfcc, axis=0)\n",
        "        var = np.var(mfcc,axis=0)\n",
        "        n_mfcc = (mfcc - mean)/var\n",
        "        self.mfccs.append(n_mfcc)\n",
        "  \n",
        "\n",
        "    self.mfccs = np.concatenate(self.mfccs,axis =0)\n",
        "\n",
        "    self.length = len(self.mfccs)\n",
        "\n",
        "    self.mfccs = np.pad(self.mfccs, pad_width=((self.context,self.context),(0,0)), mode=\"constant\", constant_values= 0)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, ind):\n",
        "    \n",
        "\n",
        "    start= ind\n",
        "    end = ind +(2*self.context+1)\n",
        "\n",
        "    frames = self.mfccs[start:end]\n",
        "    frames = frames.flatten()\n",
        "\n",
        "    frames = torch.FloatTensor(frames)\n",
        "    return frames\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    # TODO: Create a test dataset class similar to the previous class but you dont have transcripts for this\n",
        "    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNacQ8bpt9nw"
      },
      "source": [
        "# Parameters Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE7tsinAuLNy"
      },
      "source": [
        "Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmKwlFqgt_Zq"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'epochs': 70,\n",
        "    'batch_size' : 4096,\n",
        "    'context' : 30,\n",
        "    'learning_rate' : 0.001,\n",
        "    'architecture' : 'medium-cutoff'\n",
        "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mlwaKlDt_2c"
      },
      "source": [
        "# Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xi7V8x8W9z4"
      },
      "outputs": [],
      "source": [
        "train_data = AudioDataset(data_path='/content/data',context=config['context'], offset=0, partition= \"train\") #TODO: Create a dataset object using the AudioDataset class for the training data \n",
        "\n",
        "val_data = AudioDataset(data_path='/content/data',context=config['context'], offset=0, partition= \"dev\") # TODO: Create a dataset object using the AudioDataset class for the validation data \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0izuH3aQNtM",
        "outputId": "6b75c2e9-32b1-49a8-b772-659097722192"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "        34, 35, 36, 37, 38, 39]),\n",
              " array([396427,  33196,  55212,  88799,  27736,  21036,  47481,  22436,\n",
              "         14144,  48215,  28445,  42820,  56102,  38939,  35969,  13627,\n",
              "         26904,  63752,  81199,   9396,  46601,  73571,  43783,  85424,\n",
              "         21730,  31452,   3752,  31371,  59326, 111876,  20220,  84166,\n",
              "         11322,   5713,  20977,  24287,  37897,  11732,  59360,   1161]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a=val_data.transcripts\n",
        "np.unique(a, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Axui0RRYnSwW"
      },
      "outputs": [],
      "source": [
        "test_data = AudioTestDataset(data_path='/content/data',context=config['context'], offset=0, partition= \"test\") # TODO: Create a dataset object using the AudioTestDataset class for the test data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "199f7ba0-8ba1-4b5d-fdae-4c8b804e61d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  4096\n",
            "Context:  30\n",
            "Input size:  915\n",
            "Output symbols:  42\n",
            "Train dataset samples = 36191134, batches = 8836\n",
            "Validation dataset samples = 1937496, batches = 474\n",
            "Test dataset samples = 1943253, batches = 475\n"
          ]
        }
      ],
      "source": [
        "# Define dataloaders for train, val and test datasets\n",
        "# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n",
        "train_loader = torch.utils.data.DataLoader(train_data, num_workers= 4,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle= True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_data, num_workers= 2,\n",
        "                                         batch_size=config['batch_size'], pin_memory= True,\n",
        "                                         shuffle= False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, num_workers= 2, \n",
        "                                          batch_size=config['batch_size'], pin_memory= True, \n",
        "                                          shuffle= False)\n",
        "\n",
        "\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Context: \", config['context'])\n",
        "print(\"Input size: \", (2*config['context']+1)*15)\n",
        "print(\"Output symbols: \", len(PHONEMES))\n",
        "\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-GV3UvgLSoF",
        "outputId": "9403c718-7a80-4b99-d900-13e4ce0b152d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4096, 915]) torch.Size([4096])\n"
          ]
        }
      ],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(train_loader):\n",
        "    frames, phoneme = data\n",
        "    print(frames.shape, phoneme.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxjwve20JRJ2"
      },
      "source": [
        "# Network Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NJzT-mRw6iy"
      },
      "source": [
        "This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvcpontXQq9j"
      },
      "outputs": [],
      "source": [
        "# This architecture will make you cross the very low cutoff\n",
        "# However, you need to run a lot of experiments to cross the medium or high cutoff\n",
        "class Network(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, context):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        input_size = (2*context + 1) * 15 #Why is this the case?\n",
        "        output_size = 40 #Why?\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 2048),\n",
        "            torch.nn.Linear(2048,2048),\n",
        "            torch.nn.BatchNorm1d(2048),\n",
        "            torch.nn.Dropout(0.25),\n",
        "            torch.nn.GELU(),\n",
        "\n",
        "            torch.nn.Linear(2048,2048),\n",
        "            torch.nn.BatchNorm1d(2048),\n",
        "            torch.nn.Dropout(0.25),\n",
        "            torch.nn.GELU(),\n",
        "\n",
        "\n",
        "            torch.nn.Linear(2048,2048),\n",
        "            torch.nn.BatchNorm1d(2048),\n",
        "            torch.nn.Dropout(0.25),\n",
        "            torch.nn.GELU(),\n",
        "\n",
        "            torch.nn.Linear(2048,1024),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.Dropout(0.25),\n",
        "            torch.nn.GELU(),\n",
        "\n",
        "            torch.nn.Linear(1024,1024),torch.nn.BatchNorm1d(1024),torch.nn.Dropout(0.25),torch.nn.GELU(),\n",
        "            torch.nn.Linear(1024,1024),torch.nn.BatchNorm1d(1024),torch.nn.Dropout(0.25),torch.nn.GELU(),\n",
        "            torch.nn.Linear(1024,1024),torch.nn.BatchNorm1d(1024),torch.nn.Dropout(0.25),torch.nn.GELU(),\n",
        "            torch.nn.Linear(1024,output_size)\n",
        "        )      \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HejoSXe3vMVU"
      },
      "source": [
        "# Define Model, Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAhGBH7-xxth"
      },
      "source": [
        "Here we define the model, loss function, optimizer and optionally a learning rate scheduler. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk9y7xRPJ5Gp"
      },
      "outputs": [],
      "source": [
        "model = torch.load('./model_checkpoint_1.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_qtrEM1ZvLje",
        "outputId": "9efe8657-8eb8-4d79-d233-5211db4e28d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================\n",
            "                         Kernel Shape  Output Shape     Params  Mult-Adds\n",
            "Layer                                                                    \n",
            "0_model.Linear_0          [915, 2048]  [4096, 2048]  1.875968M   1.87392M\n",
            "1_model.Linear_1         [2048, 2048]  [4096, 2048]  4.196352M  4.194304M\n",
            "2_model.BatchNorm1d_2          [2048]  [4096, 2048]     4.096k     2.048k\n",
            "3_model.Dropout_3                   -  [4096, 2048]          -          -\n",
            "4_model.GELU_4                      -  [4096, 2048]          -          -\n",
            "5_model.Linear_5         [2048, 2048]  [4096, 2048]  4.196352M  4.194304M\n",
            "6_model.BatchNorm1d_6          [2048]  [4096, 2048]     4.096k     2.048k\n",
            "7_model.Dropout_7                   -  [4096, 2048]          -          -\n",
            "8_model.GELU_8                      -  [4096, 2048]          -          -\n",
            "9_model.Linear_9         [2048, 2048]  [4096, 2048]  4.196352M  4.194304M\n",
            "10_model.BatchNorm1d_10        [2048]  [4096, 2048]     4.096k     2.048k\n",
            "11_model.Dropout_11                 -  [4096, 2048]          -          -\n",
            "12_model.GELU_12                    -  [4096, 2048]          -          -\n",
            "13_model.Linear_13       [2048, 1024]  [4096, 1024]  2.098176M  2.097152M\n",
            "14_model.BatchNorm1d_14        [1024]  [4096, 1024]     2.048k     1.024k\n",
            "15_model.Dropout_15                 -  [4096, 1024]          -          -\n",
            "16_model.GELU_16                    -  [4096, 1024]          -          -\n",
            "17_model.Linear_17       [1024, 1024]  [4096, 1024]    1.0496M  1.048576M\n",
            "18_model.BatchNorm1d_18        [1024]  [4096, 1024]     2.048k     1.024k\n",
            "19_model.Dropout_19                 -  [4096, 1024]          -          -\n",
            "20_model.GELU_20                    -  [4096, 1024]          -          -\n",
            "21_model.Linear_21       [1024, 1024]  [4096, 1024]    1.0496M  1.048576M\n",
            "22_model.BatchNorm1d_22        [1024]  [4096, 1024]     2.048k     1.024k\n",
            "23_model.Dropout_23                 -  [4096, 1024]          -          -\n",
            "24_model.GELU_24                    -  [4096, 1024]          -          -\n",
            "25_model.Linear_25       [1024, 1024]  [4096, 1024]    1.0496M  1.048576M\n",
            "26_model.BatchNorm1d_26        [1024]  [4096, 1024]     2.048k     1.024k\n",
            "27_model.Dropout_27                 -  [4096, 1024]          -          -\n",
            "28_model.GELU_28                    -  [4096, 1024]          -          -\n",
            "29_model.Linear_29         [1024, 40]    [4096, 40]      41.0k     40.96k\n",
            "-------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params           19.77348M\n",
            "Trainable params       19.77348M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds             19.750912M\n",
            "=========================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c67de452-6dab-4ba6-96d0-9e47b6f4f2da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_model.Linear_0</th>\n",
              "      <td>[915, 2048]</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>1875968.0</td>\n",
              "      <td>1873920.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_model.Linear_1</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_model.BatchNorm1d_2</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_model.Dropout_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_model.GELU_4</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_model.Linear_5</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_model.BatchNorm1d_6</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_model.Dropout_7</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_model.GELU_8</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_model.Linear_9</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_model.BatchNorm1d_10</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_model.Dropout_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_model.GELU_12</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_model.Linear_13</th>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>2098176.0</td>\n",
              "      <td>2097152.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_model.BatchNorm1d_14</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_model.Dropout_15</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_model.GELU_16</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_model.Linear_17</th>\n",
              "      <td>[1024, 1024]</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>1049600.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_model.BatchNorm1d_18</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_model.Dropout_19</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_model.GELU_20</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_model.Linear_21</th>\n",
              "      <td>[1024, 1024]</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>1049600.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_model.BatchNorm1d_22</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_model.Dropout_23</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_model.GELU_24</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_model.Linear_25</th>\n",
              "      <td>[1024, 1024]</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>1049600.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_model.BatchNorm1d_26</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_model.Dropout_27</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28_model.GELU_28</th>\n",
              "      <td>-</td>\n",
              "      <td>[4096, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29_model.Linear_29</th>\n",
              "      <td>[1024, 40]</td>\n",
              "      <td>[4096, 40]</td>\n",
              "      <td>41000.0</td>\n",
              "      <td>40960.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c67de452-6dab-4ba6-96d0-9e47b6f4f2da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c67de452-6dab-4ba6-96d0-9e47b6f4f2da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c67de452-6dab-4ba6-96d0-9e47b6f4f2da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                         Kernel Shape  Output Shape     Params  Mult-Adds\n",
              "Layer                                                                    \n",
              "0_model.Linear_0          [915, 2048]  [4096, 2048]  1875968.0  1873920.0\n",
              "1_model.Linear_1         [2048, 2048]  [4096, 2048]  4196352.0  4194304.0\n",
              "2_model.BatchNorm1d_2          [2048]  [4096, 2048]     4096.0     2048.0\n",
              "3_model.Dropout_3                   -  [4096, 2048]        NaN        NaN\n",
              "4_model.GELU_4                      -  [4096, 2048]        NaN        NaN\n",
              "5_model.Linear_5         [2048, 2048]  [4096, 2048]  4196352.0  4194304.0\n",
              "6_model.BatchNorm1d_6          [2048]  [4096, 2048]     4096.0     2048.0\n",
              "7_model.Dropout_7                   -  [4096, 2048]        NaN        NaN\n",
              "8_model.GELU_8                      -  [4096, 2048]        NaN        NaN\n",
              "9_model.Linear_9         [2048, 2048]  [4096, 2048]  4196352.0  4194304.0\n",
              "10_model.BatchNorm1d_10        [2048]  [4096, 2048]     4096.0     2048.0\n",
              "11_model.Dropout_11                 -  [4096, 2048]        NaN        NaN\n",
              "12_model.GELU_12                    -  [4096, 2048]        NaN        NaN\n",
              "13_model.Linear_13       [2048, 1024]  [4096, 1024]  2098176.0  2097152.0\n",
              "14_model.BatchNorm1d_14        [1024]  [4096, 1024]     2048.0     1024.0\n",
              "15_model.Dropout_15                 -  [4096, 1024]        NaN        NaN\n",
              "16_model.GELU_16                    -  [4096, 1024]        NaN        NaN\n",
              "17_model.Linear_17       [1024, 1024]  [4096, 1024]  1049600.0  1048576.0\n",
              "18_model.BatchNorm1d_18        [1024]  [4096, 1024]     2048.0     1024.0\n",
              "19_model.Dropout_19                 -  [4096, 1024]        NaN        NaN\n",
              "20_model.GELU_20                    -  [4096, 1024]        NaN        NaN\n",
              "21_model.Linear_21       [1024, 1024]  [4096, 1024]  1049600.0  1048576.0\n",
              "22_model.BatchNorm1d_22        [1024]  [4096, 1024]     2048.0     1024.0\n",
              "23_model.Dropout_23                 -  [4096, 1024]        NaN        NaN\n",
              "24_model.GELU_24                    -  [4096, 1024]        NaN        NaN\n",
              "25_model.Linear_25       [1024, 1024]  [4096, 1024]  1049600.0  1048576.0\n",
              "26_model.BatchNorm1d_26        [1024]  [4096, 1024]     2048.0     1024.0\n",
              "27_model.Dropout_27                 -  [4096, 1024]        NaN        NaN\n",
              "28_model.GELU_28                    -  [4096, 1024]        NaN        NaN\n",
              "29_model.Linear_29         [1024, 40]    [4096, 40]    41000.0    40960.0"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_size = 15*(2*config['context'] + 1)\n",
        "model = Network(config['context']).to(device)\n",
        "frames,phoneme = next(iter(train_loader))\n",
        "# Check number of parameters of your network - Remember, you are limited to 20 million parameters for HW1 (including ensembles)\n",
        "summary(model, frames.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UROGEVJevKD-"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import StepLR,CosineAnnealingLR,ReduceLROnPlateau,ExponentialLR\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() #Defining Loss function \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate']) #Defining Optimizer\n",
        "scheduler = CosineAnnealingLR(optimizer,T_max=5,eta_min=1e-5)\n",
        "# Recommended : Define Scheduler for Learning Rate, including but not limited to StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, etc. \n",
        "# You can refer to Pytorch documentation for more information on how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training and Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JgeNhx4x2-P"
      },
      "source": [
        "This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XblOHEVtKab2",
        "outputId": "434b3e77-05d3-4deb-e4b1-ee6555637d90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "765"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wjPz7DHqKcL"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, dataloader):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0 #Monitoring Loss\n",
        "    \n",
        "    for iter, (mfccs, phonemes) in tqdm(enumerate(dataloader)):\n",
        "\n",
        "        ### Move Data to Device (Ideally GPU)\n",
        "        mfccs = mfccs.to(device)\n",
        "        phonemes = phonemes.to(device)\n",
        "\n",
        "        ### Forward Propagation\n",
        "        logits = model(mfccs)\n",
        "\n",
        "        ### Loss Calculation\n",
        "        loss = criterion(logits, phonemes)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ### Backward Propagation\n",
        "        loss.backward()\n",
        "\n",
        "        ### Gradient Descent\n",
        "        optimizer.step()\n",
        "  \n",
        "    train_loss /= len(dataloader)\n",
        "    return train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5npQNFH315V"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "\n",
        "    phone_true_list = []\n",
        "    phone_pred_list = []\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "\n",
        "        frames, phonemes = data\n",
        "        ### Move data to device (ideally GPU)\n",
        "        frames, phonemes = frames.to(device), phonemes.to(device) \n",
        "\n",
        "        with torch.inference_mode(): # makes sure that there are no gradients computed as we are not training the model now\n",
        "            ### Forward Propagation\n",
        "            logits = model(frames)\n",
        "\n",
        "        ### Get Predictions\n",
        "        predicted_phonemes = torch.argmax(logits, dim=1)\n",
        "        \n",
        "        ### Store Pred and True Labels\n",
        "        phone_pred_list.extend(predicted_phonemes.cpu().tolist())\n",
        "        phone_true_list.extend(phonemes.cpu().tolist())\n",
        "        \n",
        "        # Do you think we need loss.backward() and optimizer.step() here?\n",
        "    \n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    ### Calculate Accuracy\n",
        "    accuracy = sklearn.metrics.accuracy_score(phone_pred_list, phone_true_list) \n",
        "    return accuracy*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMd_XxPku5qp"
      },
      "source": [
        "# Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIbhR1wwbgI"
      },
      "source": [
        "This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb. \n",
        "\n",
        "We have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCDYx5VEu6qI",
        "outputId": "ce777e83-8a86-4e0e-8d68-af956d2ac04d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"0f42a1910060e4a99487f7b1241e7d8a3d45182b\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "2a1bab8bd8cd41bda5dc1c7d07a236b7",
            "6b289f83f4ac49b6b5ae7128da100b29",
            "dce1f7256ad943b29427bba6ea3f20d5",
            "b62e791320ae4845b638acb70b871eb2",
            "4f3f780c907c46dbb0988a3d835b1a64",
            "cd07cc1011b24f7bb02fc04683ac7418",
            "31d555b501e34f20b0b9d80272b818cd",
            "d8bc5c0b2a7640708341741183280abe"
          ]
        },
        "id": "xvUnYd3Bw2up",
        "outputId": "ddff15bd-1bbe-4ba2-a815-fba8716a44b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:1um2st6c) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a1bab8bd8cd41bda5dc1c7d07a236b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>▁</td></tr><tr><td>train loss</td><td>▁</td></tr><tr><td>validation accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>0.003</td></tr><tr><td>train loss</td><td>0.81942</td></tr><tr><td>validation accuracy</td><td>78.42143</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">already 85.1 Exponential saved dc deepgelu 2048 with batch norm and dropout</strong>: <a href=\"https://wandb.ai/ram-kasi/hw1p2/runs/1um2st6c\" target=\"_blank\">https://wandb.ai/ram-kasi/hw1p2/runs/1um2st6c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220929_203113-1um2st6c/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:1um2st6c). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220929_204817-3i2yuamo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ram-kasi/hw1p2/runs/3i2yuamo\" target=\"_blank\">already 85.1 Exponential saved dc deepgelu 2048 with batch norm and dropout</a></strong> to <a href=\"https://wandb.ai/ram-kasi/hw1p2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name = \"already 85.1 Exponential saved dc deepgelu 2048 with batch norm and dropout\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "    reinit=True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    project=\"hw1p2\", ### Project should be created in your wandb account \n",
        "    config=config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wft15E_IxYFi",
        "outputId": "55e74ad3-ab1a-45b5-af0f-b328728dcd73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/wandb/run-20220929_204817-3i2yuamo/files/model_arch.txt']"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Save your model architecture as a string with str(model) \n",
        "model_arch = str(model)\n",
        "\n",
        "### Save it in a txt file \n",
        "arch_file = open(\"model_arch.txt\", \"w\")\n",
        "file_write = arch_file.write(model_arch)\n",
        "arch_file.close()\n",
        "\n",
        "### log it in your wandb run with wandb.save()\n",
        "wandb.save('model_arch.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFGsFGDRMPjP",
        "outputId": "8b48bece-fa91-4fb8-afec-310f3a21f7e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer.param_groups[0]['lr']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nclx_04fu7Dd"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdLMWfEpyGOB"
      },
      "source": [
        "Now, it is time to finally run your ablations! Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG4F77Nm0Am9",
        "outputId": "1013d479-807f-4c12-b239-13af4d4838ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3572\n",
            "\tValidation Accuracy: 85.06%\n",
            "\tLearning Rate: 0.00100000\n",
            "\n",
            "Epoch 2/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3545\n",
            "\tValidation Accuracy: 85.07%\n",
            "\tLearning Rate: 0.00001031\n",
            "\n",
            "Epoch 3/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3540\n",
            "\tValidation Accuracy: 85.07%\n",
            "\tLearning Rate: 0.00001043\n",
            "\n",
            "Epoch 4/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3540\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001042\n",
            "\n",
            "Epoch 5/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3537\n",
            "\tValidation Accuracy: 85.07%\n",
            "\tLearning Rate: 0.00001062\n",
            "\n",
            "Epoch 6/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3535\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001053\n",
            "\n",
            "Epoch 7/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3536\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001061\n",
            "\n",
            "Epoch 8/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3535\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001056\n",
            "\n",
            "Epoch 9/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3533\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001069\n",
            "\n",
            "Epoch 10/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3534\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001068\n",
            "\n",
            "Epoch 11/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3533\n",
            "\tValidation Accuracy: 85.07%\n",
            "\tLearning Rate: 0.00001061\n",
            "\n",
            "Epoch 12/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3532\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001045\n",
            "\n",
            "Epoch 13/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3532\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001055\n",
            "\n",
            "Epoch 14/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3530\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001058\n",
            "\n",
            "Epoch 15/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3531\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001063\n",
            "\n",
            "Epoch 16/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3530\n",
            "\tValidation Accuracy: 85.07%\n",
            "\tLearning Rate: 0.00001056\n",
            "\n",
            "Epoch 17/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3532\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001053\n",
            "\n",
            "Epoch 18/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3529\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001055\n",
            "\n",
            "Epoch 19/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3529\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001064\n",
            "\n",
            "Epoch 20/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3529\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001061\n",
            "\n",
            "Epoch 21/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:05, 13.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3529\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001070\n",
            "\n",
            "Epoch 22/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3529\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001063\n",
            "\n",
            "Epoch 23/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3527\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001058\n",
            "\n",
            "Epoch 24/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3527\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001063\n",
            "\n",
            "Epoch 25/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3528\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001062\n",
            "\n",
            "Epoch 26/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3527\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001065\n",
            "\n",
            "Epoch 27/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3527\n",
            "\tValidation Accuracy: 85.09%\n",
            "\tLearning Rate: 0.00001061\n",
            "\n",
            "Epoch 28/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3528\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001071\n",
            "\n",
            "Epoch 29/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3527\n",
            "\tValidation Accuracy: 85.08%\n",
            "\tLearning Rate: 0.00001070\n",
            "\n",
            "Epoch 30/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8836it [11:06, 13.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.3527\n",
            "\tValidation Accuracy: 85.07%\n",
            "\tLearning Rate: 0.00001056\n",
            "\n",
            "Epoch 31/70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "8705it [10:52, 13.33it/s]"
          ]
        }
      ],
      "source": [
        "from torch.optim import adam\n",
        "from tqdm import tqdm\n",
        "# Iterate over number of epochs to train and evaluate your model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "best_acc = 85.5 ### Monitor best accuracy in your run\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    train_loss = train(model, optimizer, criterion, train_loader)\n",
        "    accuracy = eval(model, val_loader)\n",
        "    learning_rate = optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\tTrain Loss: {:.4f}\".format(train_loss))\n",
        "    print(\"\\tValidation Accuracy: {:.2f}%\".format(accuracy))\n",
        "    print(\"\\tLearning Rate: {:.8f}\".format(learning_rate))\n",
        "\n",
        "    scheduler.step(accuracy) #or on val_acc with max in scheudler\n",
        "\n",
        "    ### Log metrics at each epoch in your run - Optionally, you can log at each batch inside train/eval functions (explore wandb documentation/wandb recitation)\n",
        "    wandb.log({\"train loss\": train_loss, \"validation accuracy\": accuracy, \"Learning Rate\": learning_rate})\n",
        "\n",
        "    ### Save checkpoint if accuracy is better than your current best\n",
        "    if accuracy >= best_acc:\n",
        "\n",
        "      ### Save checkpoint with information you want\n",
        "      torch.save({'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': train_loss,\n",
        "              'acc': accuracy}, \n",
        "        './model_checkpoint_checker.pth')\n",
        "      \n",
        "      ### Save checkpoint in wandb\n",
        "      wandb.save('checkpoint_checker.pth')\n",
        " # scheduler.step(accuracy) #or on val_acc with max in scheudler\n",
        "\n",
        "    # Is your training time very high? Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it \n",
        "    # Refer - https://pytorch.org/docs/stable/notes/amp_examples.html\n",
        "\n",
        "### Finish your wandb run\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UULQ-YPMGwr_"
      },
      "outputs": [],
      "source": [
        "torch.save(model,'./model_checkpoint_2.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kXwf5YUo_4A"
      },
      "source": [
        "# Testing and submission to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1hSFYLpJvH"
      },
      "source": [
        "Before we get to the following code, make sure to see the format of submission given in *random_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-SU9fZ3xHtk"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "  ### What you call for model to perform inference?\n",
        "  model.eval()\n",
        "\n",
        "  ### List to store predicted phonemes of test data\n",
        "  test_predictions = []\n",
        "\n",
        "  ### Which mode do you need to avoid gradients?\n",
        "  with torch.no_grad():\n",
        "\n",
        "      test_predictions= [ ]\n",
        "      \n",
        "\n",
        "      for i, frames in enumerate(tqdm(test_loader)):\n",
        "\n",
        "          frames = frames.float().to(device)             \n",
        "          \n",
        "          output = model(frames)\n",
        "        \n",
        "\n",
        "          ### Get most likely predicted phoneme with argmax\n",
        "          predicted_phonemes = output.argmax(dim=1)\n",
        "          \n",
        "          test_predictions.append(predicted_phonemes)\n",
        "\n",
        "  prediction = torch.cat(test_predictions,0)\n",
        "\n",
        "          ### How do you store predicted_phonemes with test_predictions? Hint, look at eval \n",
        "          \n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG9v6Xmxu7wp",
        "outputId": "c67b0063-a054-48e4-d668-f37f00c2ac2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 475/475 [00:13<00:00, 36.31it/s]\n"
          ]
        }
      ],
      "source": [
        "predictions = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4XbAKzqA6YE"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE1hRnvf0bFz"
      },
      "outputs": [],
      "source": [
        "### Create CSV file with predictions\n",
        "with open(\"./submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(predictions)):\n",
        "        f.write(\"{},{}\\n\".format(i, predictions[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjcammuCxMKN",
        "outputId": "a0afe2fd-3257-478a-9230-f9caa4af7772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 18.6M/18.6M [00:05<00:00, 3.44MB/s]\n",
            "Successfully submitted to Frame-Level Speech Recognition"
          ]
        }
      ],
      "source": [
        "### Submit to kaggle competition using kaggle API\n",
        "!kaggle competitions submit -c 11-785-f22-hw1p2 -f ./submission.csv -m \"Test Submission\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "yMd_XxPku5qp",
        "nclx_04fu7Dd",
        "_kXwf5YUo_4A"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2a1bab8bd8cd41bda5dc1c7d07a236b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b289f83f4ac49b6b5ae7128da100b29",
              "IPY_MODEL_dce1f7256ad943b29427bba6ea3f20d5"
            ],
            "layout": "IPY_MODEL_b62e791320ae4845b638acb70b871eb2"
          }
        },
        "31d555b501e34f20b0b9d80272b818cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f3f780c907c46dbb0988a3d835b1a64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b289f83f4ac49b6b5ae7128da100b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f3f780c907c46dbb0988a3d835b1a64",
            "placeholder": "​",
            "style": "IPY_MODEL_cd07cc1011b24f7bb02fc04683ac7418",
            "value": "0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "b62e791320ae4845b638acb70b871eb2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd07cc1011b24f7bb02fc04683ac7418": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8bc5c0b2a7640708341741183280abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dce1f7256ad943b29427bba6ea3f20d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31d555b501e34f20b0b9d80272b818cd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8bc5c0b2a7640708341741183280abe",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}